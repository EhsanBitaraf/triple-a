{
    "_default": {
        "1": {
            "Abstract": "Integrating large language models (LLMs) into healthcare presents potential but faces challenges. Directly pre-training LLMs for domains like medicine is resource-heavy and sometimes unfeasible. Sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain specific insights. Addressing these challenges, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). A notable contribution of our study is the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and SFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing Baichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores 16.66 in BLEU-1 and 27.44 in ROUGE1, outperforming the SFT's 12.69 and 24.21. This highlights the strength of our training approach in refining LLMs for medical applications.",
            "ArxivID": "2310.09089v1",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Qichen Ye",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Junling Liu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Dading Chong",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Peilin Zhou",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Yining Hua",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Andrew Liu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Qichen Ye"
                    },
                    {
                        "name": "Junling Liu"
                    },
                    {
                        "name": "Dading Chong"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Andrew Liu"
                    }
                ],
                "category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL"
                },
                "id": "http://arxiv.org/abs/2310.09089v1",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2310.09089v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2310.09089v1",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-10-13T13:17:03Z",
                "summary": "Integrating large language models (LLMs) into healthcare presents potential\nbut faces challenges. Directly pre-training LLMs for domains like medicine is\nresource-heavy and sometimes unfeasible. Sole reliance on Supervised\nFine-tuning (SFT) can result in overconfident predictions and may not tap into\ndomain specific insights. Addressing these challenges, we present a multi-stage\ntraining method combining Domain-specific Continued Pre-training (DCPT), SFT,\nand Direct Preference Optimization (DPO). A notable contribution of our study\nis the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing\nmedical question answering, plain texts, knowledge graphs, and dialogues,\nsegmented into three training stages. The medical LLM trained with our\npipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and\nSFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing\nBaichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores\n16.66 in BLEU-1 and 27.44 in ROUGE1, outperforming the SFT's 12.69 and 24.21.\nThis highlights the strength of our training approach in refining LLMs for\nmedical applications.",
                "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large\n  Language Model",
                "updated": "2023-10-13T13:17:03Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-10-13T13:17:03Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large   Language Model",
            "Topics": null
        },
        "2": {
            "Abstract": "Few-shot learning has been studied to adapt models to tasks with very few samples. It holds profound significance, particularly in clinical tasks, due to the high annotation cost of medical images. Several works have explored few-shot learning on medical images, yet they still require a large number of medical images for pre-training models to gain domain-specific priors. Vision foundation models recently have achieved remarkable success in natural images. Hence, adapting rapidly advancing vision foundation models from natural images to few-shot clinical tasks holds great promise. MedFMC has recently organized a challenge to shed more light on this topic at NeurIPS 2023. In this work, we present our challenge solution. We observe that a simple variant of fine-tuning with partial freezing shows remarkable performance. Empirical evidence demonstrates that this approach could outperform various common fine-tuning methods under limited sample sizes. Additionally, we explore enhanced utilization of semantic supervision to boost performance. We propose a novel approach that contextualizes labels via large language models (LLMs). Our findings reveal that the context generated by LLMs significantly enhances the discrimination of semantic embeddings for similar categories, resulting in a notable performance improvement of 3%-5% in 1-shot settings compared to commonly employed one-hot labels and other semantic supervision methods. Our solution secures the 1st place in the MedFMC challenge.",
            "ArxivID": "2312.07125v1",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Kaipeng Zheng",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Weiran Huang",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Lichao Sun",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CV",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Kaipeng Zheng"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CV"
                },
                "id": "http://arxiv.org/abs/2312.07125v1",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2312.07125v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2312.07125v1",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-12-12T09:58:07Z",
                "summary": "Few-shot learning has been studied to adapt models to tasks with very few\nsamples. It holds profound significance, particularly in clinical tasks, due to\nthe high annotation cost of medical images. Several works have explored\nfew-shot learning on medical images, yet they still require a large number of\nmedical images for pre-training models to gain domain-specific priors. Vision\nfoundation models recently have achieved remarkable success in natural images.\nHence, adapting rapidly advancing vision foundation models from natural images\nto few-shot clinical tasks holds great promise. MedFMC has recently organized a\nchallenge to shed more light on this topic at NeurIPS 2023. In this work, we\npresent our challenge solution. We observe that a simple variant of fine-tuning\nwith partial freezing shows remarkable performance. Empirical evidence\ndemonstrates that this approach could outperform various common fine-tuning\nmethods under limited sample sizes. Additionally, we explore enhanced\nutilization of semantic supervision to boost performance. We propose a novel\napproach that contextualizes labels via large language models (LLMs). Our\nfindings reveal that the context generated by LLMs significantly enhances the\ndiscrimination of semantic embeddings for similar categories, resulting in a\nnotable performance improvement of 3%-5% in 1-shot settings compared to\ncommonly employed one-hot labels and other semantic supervision methods. Our\nsolution secures the 1st place in the MedFMC challenge.",
                "title": "Efficient Few-Shot Clinical Task Adaptation with Large Language Models",
                "updated": "2023-12-12T09:58:07Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-12-12T09:58:07Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Efficient Few-Shot Clinical Task Adaptation with Large Language Models",
            "Topics": null
        },
        "3": {
            "Abstract": "An accurate differential diagnosis (DDx) is a cornerstone of medical care, often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by Large Language Models (LLMs) present new opportunities to both assist and automate aspects of this process. In this study, we introduce an LLM optimized for diagnostic reasoning, and evaluate its ability to generate a DDx alone or as an aid to clinicians. 20 clinicians evaluated 302 challenging, real-world medical cases sourced from the New England Journal of Medicine (NEJM) case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: either assistance from search engines and standard medical resources, or LLM assistance in addition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective assistive tools. Our LLM for DDx exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% vs 33.6%, [p = 0.04]). Comparing the two assisted study arms, the DDx quality score was higher for clinicians assisted by our LLM (top-10 accuracy 51.7%) compared to clinicians without its assistance (36.1%) (McNemar's Test: 45.7, p < 0.01) and clinicians with search (44.4%) (4.75, p = 0.03). Further, clinicians assisted by our LLM arrived at more comprehensive differential lists than those without its assistance. Our study suggests that our LLM for DDx has potential to improve clinicians' diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients' access to specialist-level expertise.",
            "ArxivID": "2312.00164v1",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Daniel McDuff",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Mike Schaekermann",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Tao Tu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Anil Palepu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Amy Wang",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Jake Garrison",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Karan Singhal",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Yash Sharma",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Shekoofeh Azizi",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Kavita Kulkarni",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Le Hou",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Yong Cheng",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Yun Liu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "S Sara Mahdavi",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Sushant Prakash",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Anupam Pathak",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Christopher Semturs",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Shwetak Patel",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Dale R Webster",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Ewa Dominowska",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Juraj Gottweis",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Joelle Barral",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Katherine Chou",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Greg S Corrado",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Yossi Matias",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Jake Sunshine",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Alan Karthikesalingam",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Vivek Natarajan",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CY",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Mike Schaekermann"
                    },
                    {
                        "name": "Tao Tu"
                    },
                    {
                        "name": "Anil Palepu"
                    },
                    {
                        "name": "Amy Wang"
                    },
                    {
                        "name": "Jake Garrison"
                    },
                    {
                        "name": "Karan Singhal"
                    },
                    {
                        "name": "Yash Sharma"
                    },
                    {
                        "name": "Shekoofeh Azizi"
                    },
                    {
                        "name": "Kavita Kulkarni"
                    },
                    {
                        "name": "Le Hou"
                    },
                    {
                        "name": "Yong Cheng"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "S Sara Mahdavi"
                    },
                    {
                        "name": "Sushant Prakash"
                    },
                    {
                        "name": "Anupam Pathak"
                    },
                    {
                        "name": "Christopher Semturs"
                    },
                    {
                        "name": "Shwetak Patel"
                    },
                    {
                        "name": "Dale R Webster"
                    },
                    {
                        "name": "Ewa Dominowska"
                    },
                    {
                        "name": "Juraj Gottweis"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Katherine Chou"
                    },
                    {
                        "name": "Greg S Corrado"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Jake Sunshine"
                    },
                    {
                        "name": "Alan Karthikesalingam"
                    },
                    {
                        "name": "Vivek Natarajan"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CY"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.AI"
                    }
                ],
                "id": "http://arxiv.org/abs/2312.00164v1",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2312.00164v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2312.00164v1",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-11-30T19:55:51Z",
                "summary": "An accurate differential diagnosis (DDx) is a cornerstone of medical care,\noften reached through an iterative process of interpretation that combines\nclinical history, physical examination, investigations and procedures.\nInteractive interfaces powered by Large Language Models (LLMs) present new\nopportunities to both assist and automate aspects of this process. In this\nstudy, we introduce an LLM optimized for diagnostic reasoning, and evaluate its\nability to generate a DDx alone or as an aid to clinicians. 20 clinicians\nevaluated 302 challenging, real-world medical cases sourced from the New\nEngland Journal of Medicine (NEJM) case reports. Each case report was read by\ntwo clinicians, who were randomized to one of two assistive conditions: either\nassistance from search engines and standard medical resources, or LLM\nassistance in addition to these tools. All clinicians provided a baseline,\nunassisted DDx prior to using the respective assistive tools. Our LLM for DDx\nexhibited standalone performance that exceeded that of unassisted clinicians\n(top-10 accuracy 59.1% vs 33.6%, [p = 0.04]). Comparing the two assisted study\narms, the DDx quality score was higher for clinicians assisted by our LLM\n(top-10 accuracy 51.7%) compared to clinicians without its assistance (36.1%)\n(McNemar's Test: 45.7, p < 0.01) and clinicians with search (44.4%) (4.75, p =\n0.03). Further, clinicians assisted by our LLM arrived at more comprehensive\ndifferential lists than those without its assistance. Our study suggests that\nour LLM for DDx has potential to improve clinicians' diagnostic reasoning and\naccuracy in challenging cases, meriting further real-world evaluation for its\nability to empower physicians and widen patients' access to specialist-level\nexpertise.",
                "title": "Towards Accurate Differential Diagnosis with Large Language Models",
                "updated": "2023-11-30T19:55:51Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-11-30T19:55:51Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Towards Accurate Differential Diagnosis with Large Language Models",
            "Topics": null
        },
        "4": {
            "Abstract": "Large language models (LLMs) have been applied to tasks in healthcare, ranging from medical exam questions to responding to patient questions. With increasing institutional partnerships between companies producing LLMs and healthcare systems, real world clinical application is coming closer to reality. As these models gain traction, it is essential for healthcare practitioners to understand what LLMs are, their development, their current and potential applications, and the associated pitfalls when utilized in medicine. This review and accompanying tutorial aim to give an overview of these topics to aid healthcare practitioners in understanding the rapidly changing landscape of LLMs as applied to medicine.",
            "ArxivID": "2309.00087v1",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Jesutofunmi A. Omiye",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Haiwen Gui",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Shawheen J. Rezaei",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "James Zou",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Roxana Daneshjou",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Jesutofunmi A. Omiye"
                    },
                    {
                        "name": "Haiwen Gui"
                    },
                    {
                        "name": "Shawheen J. Rezaei"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Roxana Daneshjou"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CL"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.AI"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CY"
                    }
                ],
                "id": "http://arxiv.org/abs/2309.00087v1",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2309.00087v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2309.00087v1",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-08-31T19:06:39Z",
                "summary": "Large language models (LLMs) have been applied to tasks in healthcare,\nranging from medical exam questions to responding to patient questions. With\nincreasing institutional partnerships between companies producing LLMs and\nhealthcare systems, real world clinical application is coming closer to\nreality. As these models gain traction, it is essential for healthcare\npractitioners to understand what LLMs are, their development, their current and\npotential applications, and the associated pitfalls when utilized in medicine.\nThis review and accompanying tutorial aim to give an overview of these topics\nto aid healthcare practitioners in understanding the rapidly changing landscape\nof LLMs as applied to medicine.",
                "title": "Large language models in medicine: the potentials and pitfalls",
                "updated": "2023-08-31T19:06:39Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-08-31T19:06:39Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Large language models in medicine: the potentials and pitfalls",
            "Topics": null
        },
        "5": {
            "Abstract": "A knowledge gap persists between machine learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to ChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Following the re-implementation and optimization of the published models, the head-to-head comparison of the ChatGPT ADA-crafted ML models and their respective manually crafted counterparts revealed no significant differences in traditional performance metrics (P>0.474). Strikingly, the ChatGPT ADA-crafted ML models often outperformed their counterparts. In conclusion, ChatGPT ADA offers a promising avenue to democratize ML in medicine by simplifying complex data analyses, yet should enhance, not replace, specialized training and resources, to promote broader applications in medical research and practice.",
            "ArxivID": "2308.14120v3",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Soroosh Tayebi Arasteh",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Tianyu Han",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Mahshad Lotfinia",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Christiane Kuhl",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Jakob Nikolas Kather",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Daniel Truhn",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Sven Nebelung",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.LG",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Soroosh Tayebi Arasteh"
                    },
                    {
                        "name": "Tianyu Han"
                    },
                    {
                        "name": "Mahshad Lotfinia"
                    },
                    {
                        "name": "Christiane Kuhl"
                    },
                    {
                        "name": "Jakob Nikolas Kather"
                    },
                    {
                        "name": "Daniel Truhn"
                    },
                    {
                        "name": "Sven Nebelung"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.LG"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.AI"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CL"
                    }
                ],
                "id": "http://arxiv.org/abs/2308.14120v3",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2308.14120v3",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2308.14120v3",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-08-27T14:28:38Z",
                "summary": "A knowledge gap persists between machine learning (ML) developers (e.g., data\nscientists) and practitioners (e.g., clinicians), hampering the full\nutilization of ML for clinical data analysis. We investigated the potential of\nthe ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this\ngap and perform ML analyses efficiently. Real-world clinical datasets and study\ndetails from large trials across various medical specialties were presented to\nChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed\nstate-of-the-art ML models based on the original study's training data to\npredict clinical outcomes such as cancer development, cancer progression,\ndisease complications, or biomarkers such as pathogenic gene sequences.\nFollowing the re-implementation and optimization of the published models, the\nhead-to-head comparison of the ChatGPT ADA-crafted ML models and their\nrespective manually crafted counterparts revealed no significant differences in\ntraditional performance metrics (P>0.474). Strikingly, the ChatGPT ADA-crafted\nML models often outperformed their counterparts. In conclusion, ChatGPT ADA\noffers a promising avenue to democratize ML in medicine by simplifying complex\ndata analyses, yet should enhance, not replace, specialized training and\nresources, to promote broader applications in medical research and practice.",
                "title": "Large Language Models Streamline Automated Machine Learning for Clinical\n  Studies",
                "updated": "2023-10-09T18:01:12Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-08-27T14:28:38Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Large Language Models Streamline Automated Machine Learning for Clinical   Studies",
            "Topics": null
        },
        "6": {
            "Abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of \"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "ArxivID": "2310.06271v1",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Ziwei Ji",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Tiezheng Yu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Yan Xu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Nayeon Lee",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Etsuko Ishii",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Pascale Fung",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:comment": {
                    "#text": "Accepted by the findings of EMNLP 2023",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Etsuko Ishii"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CL"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.AI"
                    }
                ],
                "id": "http://arxiv.org/abs/2310.06271v1",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2310.06271v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2310.06271v1",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-10-10T03:05:44Z",
                "summary": "Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.",
                "title": "Towards Mitigating Hallucination in Large Language Models via\n  Self-Reflection",
                "updated": "2023-10-10T03:05:44Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-10-10T03:05:44Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Towards Mitigating Hallucination in Large Language Models via   Self-Reflection",
            "Topics": null
        },
        "7": {
            "Abstract": "Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether close- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer and reason about difficult real-world-based questions. We focus on three popular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and retrieval augmentation. Based on an expert annotation of the generated CoTs, we found that InstructGPT can often read, reason and recall expert knowledge. Last, by leveraging advances in prompt engineering (few-shot and ensemble methods), we demonstrated that GPT-3.5 not only yields calibrated predictive distributions, but also reaches the passing score on three datasets: MedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are closing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.",
            "ArxivID": "2207.08143v4",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Valentin Li\u00e9vin",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Christoffer Egeberg Hother",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Andreas Geert Motzfeldt",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Ole Winther",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:comment": {
                    "#text": "37 pages, 23 figures. v1: results using InstructGPT, v2.0: added the\n  Codex experiments, v2.1: added the missing test MedMCQA results for Codex\n  5-shot CoT and using k=100 samples, v3.0: added results for open source\n  models -- ready for publication (final version)",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Valentin Li\u00e9vin"
                    },
                    {
                        "name": "Christoffer Egeberg Hother"
                    },
                    {
                        "name": "Andreas Geert Motzfeldt"
                    },
                    {
                        "name": "Ole Winther"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CL"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.AI"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.LG"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "I.2.1; I.2.7"
                    }
                ],
                "id": "http://arxiv.org/abs/2207.08143v4",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2207.08143v4",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2207.08143v4",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2022-07-17T11:24:44Z",
                "summary": "Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nclose- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer\nand reason about difficult real-world-based questions. We focus on three\npopular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple\nprompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and\nretrieval augmentation. Based on an expert annotation of the generated CoTs, we\nfound that InstructGPT can often read, reason and recall expert knowledge.\nLast, by leveraging advances in prompt engineering (few-shot and ensemble\nmethods), we demonstrated that GPT-3.5 not only yields calibrated predictive\ndistributions, but also reaches the passing score on three datasets:\nMedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are\nclosing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.",
                "title": "Can large language models reason about medical questions?",
                "updated": "2023-12-24T11:17:23Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2022-07-17T11:24:44Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Can large language models reason about medical questions?",
            "Topics": null
        },
        "8": {
            "Abstract": "Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates seven scientific/medical, three general-domain and five math word question answering datasets.",
            "ArxivID": "2301.11596v5",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Simon Ott",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Konstantin Hebenstreit",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Valentin Li\u00e9vin",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Christoffer Egeberg Hother",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Milad Moradi",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Maximilian Mayrhauser",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Robert Praas",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Ole Winther",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Matthias Samwald",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:comment": {
                    "#text": "Revision: added datasets, formatting",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Simon Ott"
                    },
                    {
                        "name": "Konstantin Hebenstreit"
                    },
                    {
                        "name": "Valentin Li\u00e9vin"
                    },
                    {
                        "name": "Christoffer Egeberg Hother"
                    },
                    {
                        "name": "Milad Moradi"
                    },
                    {
                        "name": "Maximilian Mayrhauser"
                    },
                    {
                        "name": "Robert Praas"
                    },
                    {
                        "name": "Ole Winther"
                    },
                    {
                        "name": "Matthias Samwald"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CL"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.AI"
                    }
                ],
                "id": "http://arxiv.org/abs/2301.11596v5",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2301.11596v5",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2301.11596v5",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-01-27T08:45:53Z",
                "summary": "Large language models (LLMs) such as GPT-4 have recently demonstrated\nimpressive results across a wide range of tasks. LLMs are still limited,\nhowever, in that they frequently fail at complex reasoning, their reasoning\nprocesses are opaque, they are prone to 'hallucinate' facts, and there are\nconcerns about their underlying biases. Letting models verbalize reasoning\nsteps as natural language, a technique known as chain-of-thought prompting, has\nrecently been proposed as a way to address some of these issues. Here we\npresent ThoughtSource, a meta-dataset and software library for chain-of-thought\n(CoT) reasoning. The goal of ThoughtSource is to improve future artificial\nintelligence systems by facilitating qualitative understanding of CoTs,\nenabling empirical evaluations, and providing training data. This first release\nof ThoughtSource integrates seven scientific/medical, three general-domain and\nfive math word question answering datasets.",
                "title": "ThoughtSource: A central hub for large language model reasoning data",
                "updated": "2023-07-27T09:37:35Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-01-27T08:45:53Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "ThoughtSource: A central hub for large language model reasoning data",
            "Topics": null
        },
        "9": {
            "Abstract": "Large language models (LLMs) can capture rich representations of concepts that are useful for real-world tasks. However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus. We demonstrate that with only few-shot tuning, a large language model is capable of grounding various physiological and behavioral time-series data and making meaningful inferences on numerous health tasks for both clinical and wellness contexts. Using data from wearable and medical sensor recordings, we evaluate these capabilities on the tasks of cardiac signal analysis, physical activity recognition, metabolic calculation (e.g., calories burned), and estimation of stress reports and mental health screeners.",
            "ArxivID": "2305.15525v1",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Xin Liu",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Daniel McDuff",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Geza Kovacs",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Isaac Galatzer-Levy",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Jacob Sunshine",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Jiening Zhan",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Ming-Zher Poh",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Shun Liao",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Paolo Di Achille",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Shwetak Patel",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Geza Kovacs"
                    },
                    {
                        "name": "Isaac Galatzer-Levy"
                    },
                    {
                        "name": "Jacob Sunshine"
                    },
                    {
                        "name": "Jiening Zhan"
                    },
                    {
                        "name": "Ming-Zher Poh"
                    },
                    {
                        "name": "Shun Liao"
                    },
                    {
                        "name": "Paolo Di Achille"
                    },
                    {
                        "name": "Shwetak Patel"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CL"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.LG"
                    }
                ],
                "id": "http://arxiv.org/abs/2305.15525v1",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2305.15525v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2305.15525v1",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-05-24T19:25:16Z",
                "summary": "Large language models (LLMs) can capture rich representations of concepts\nthat are useful for real-world tasks. However, language alone is limited. While\nexisting LLMs excel at text-based inferences, health applications require that\nmodels be grounded in numerical data (e.g., vital signs, laboratory values in\nclinical domains; steps, movement in the wellness domain) that is not easily or\nreadily expressed as text in existing training corpus. We demonstrate that with\nonly few-shot tuning, a large language model is capable of grounding various\nphysiological and behavioral time-series data and making meaningful inferences\non numerous health tasks for both clinical and wellness contexts. Using data\nfrom wearable and medical sensor recordings, we evaluate these capabilities on\nthe tasks of cardiac signal analysis, physical activity recognition, metabolic\ncalculation (e.g., calories burned), and estimation of stress reports and\nmental health screeners.",
                "title": "Large Language Models are Few-Shot Health Learners",
                "updated": "2023-05-24T19:25:16Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-05-24T19:25:16Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "Large Language Models are Few-Shot Health Learners",
            "Topics": null
        },
        "10": {
            "Abstract": "The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to flexibly predict psychiatric risk based on free descriptions of functioning from both patients and clinicians.",
            "ArxivID": "2308.01834v1",
            "Authors": [
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Isaac R. Galatzer-Levy",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Daniel McDuff",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Vivek Natarajan",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Alan Karthikesalingam",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                },
                {
                    "Affiliations": null,
                    "ForeName": null,
                    "FullName": "Matteo Malgaroli",
                    "HashID": null,
                    "LastName": null,
                    "ORCID": null
                }
            ],
            "CiteCrawlerDeep": 0,
            "CitedBy": null,
            "DOI": null,
            "FlagAffiliationMining": null,
            "FlagExtractKG": null,
            "FlagExtractTopic": null,
            "InsertType": null,
            "Journal": "Arxiv",
            "Keywords": null,
            "NamedEntities": null,
            "OreginalArticle": {
                "arxiv:primary_category": {
                    "@scheme": "http://arxiv.org/schemas/atom",
                    "@term": "cs.CL",
                    "@xmlns:arxiv": "http://arxiv.org/schemas/atom"
                },
                "author": [
                    {
                        "name": "Isaac R. Galatzer-Levy"
                    },
                    {
                        "name": "Daniel McDuff"
                    },
                    {
                        "name": "Vivek Natarajan"
                    },
                    {
                        "name": "Alan Karthikesalingam"
                    },
                    {
                        "name": "Matteo Malgaroli"
                    }
                ],
                "category": [
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.CL"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.AI"
                    },
                    {
                        "@scheme": "http://arxiv.org/schemas/atom",
                        "@term": "cs.LG"
                    }
                ],
                "id": "http://arxiv.org/abs/2308.01834v1",
                "link": [
                    {
                        "@href": "http://arxiv.org/abs/2308.01834v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "http://arxiv.org/pdf/2308.01834v1",
                        "@rel": "related",
                        "@title": "pdf",
                        "@type": "application/pdf"
                    }
                ],
                "published": "2023-08-03T15:52:27Z",
                "summary": "The current work investigates the capability of Large language models (LLMs)\nthat are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)\nto predict psychiatric functioning from patient interviews and clinical\ndescriptions without being trained to do so. To assess this, n = 145 depression\nand n =115 PTSD assessments and n = 46 clinical case studies across high\nprevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma\nand stress, Addictive disorders) were analyzed using prompts to extract\nestimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is\ncapable of assessing psychiatric functioning across a range of psychiatric\nconditions with the strongest performance being the prediction of depression\nscores based on standardized assessments (Accuracy range= 0.80 - 0.84) which\nwere statistically indistinguishable from human clinical raters t(1,144) =\n1.20; p = 0.23. Results show the potential for general clinical language models\nto flexibly predict psychiatric risk based on free descriptions of functioning\nfrom both patients and clinicians.",
                "title": "The Capability of Large Language Models to Measure Psychiatric\n  Functioning",
                "updated": "2023-08-03T15:52:27Z"
            },
            "PMC": null,
            "PMID": null,
            "Published": "2023-08-03T15:52:27Z",
            "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20OR%20abs%3A%22medical%22&id_list=&start=20&max_results=10",
            "ReferenceCrawlerDeep": 0,
            "References": null,
            "SourceBank": 2,
            "State": 2,
            "Title": "The Capability of Large Language Models to Measure Psychiatric   Functioning",
            "Topics": null
        }
    }
}