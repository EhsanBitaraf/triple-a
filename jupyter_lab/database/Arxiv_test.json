{"_default": {"1": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "2": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "3": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "4": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "5": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "6": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "7": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "8": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "9": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "10": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "11": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "12": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "13": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "14": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "15": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "16": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "17": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "18": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "19": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "20": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "21": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "22": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "23": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "24": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "25": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "26": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "27": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "28": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "29": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "30": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "31": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "32": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "33": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "34": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "35": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "36": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "37": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "38": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "39": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "40": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "41": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "42": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "43": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "44": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "45": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "46": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "47": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "48": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "49": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "50": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "51": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "52": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "53": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "54": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "55": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "56": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "57": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "58": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "59": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "60": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "61": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "62": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "63": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "64": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "65": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "66": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "67": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "68": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "69": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "70": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "71": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "72": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "73": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "74": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "75": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "76": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "77": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "78": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "79": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "80": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "81": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "82": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "83": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "84": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "85": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "86": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "87": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "88": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "89": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "90": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "91": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "92": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "93": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "94": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "95": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "96": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "97": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "98": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "99": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "100": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "101": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "102": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "103": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "104": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "105": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "106": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "107": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "108": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "109": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "110": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "111": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "112": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "113": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "114": {"Abstract": null, "ArxivID": "2311.15180v1", "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"arxiv:comment": {"#text": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "arxiv:primary_category": {"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR", "@xmlns:arxiv": "http://arxiv.org/schemas/atom"}, "author": {"name": "Boyang Yu"}, "category": [{"@scheme": "http://arxiv.org/schemas/atom", "@term": "q-fin.TR"}, {"@scheme": "http://arxiv.org/schemas/atom", "@term": "cs.CL"}], "id": "http://arxiv.org/abs/2311.15180v1", "link": [{"@href": "http://arxiv.org/abs/2311.15180v1", "@rel": "alternate", "@type": "text/html"}, {"@href": "http://arxiv.org/pdf/2311.15180v1", "@rel": "related", "@title": "pdf", "@type": "application/pdf"}], "published": "2023-11-26T03:54:03Z", "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.", "title": "Benchmarking Large Language Model Volatility", "updated": "2023-11-26T03:54:03Z"}, "PMC": null, "PMID": null, "Published": null, "QueryTranslation": "ArXiv Query: search_query=ti%3A%22large%20language%20model%22%20AND%20ti%3ABenchmark&id_list=&start=0&max_results=10", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 2, "State": -1, "Title": null, "Topics": null}, "115": {"Abstract": null, "ArxivID": null, "Authors": null, "CiteCrawlerDeep": 0, "CitedBy": null, "DOI": null, "FlagAffiliationMining": null, "FlagExtractKG": null, "FlagExtractTopic": null, "InsertType": null, "Journal": null, "Keywords": null, "NamedEntities": null, "OreginalArticle": {"PubmedArticleSet": {"PubmedArticle": {"MedlineCitation": {"@IndexingMethod": "Automated", "@Owner": "NLM", "@Status": "Publisher", "Article": {"@PubModel": "Print-Electronic", "Abstract": {"AbstractText": [{"#text": "To analyze the quality and readability of information regarding shoulder stabilization surgery available using an online AI software (ChatGPT), using standardized scoring systems, as well as to report on the given answers by the AI.", "@Label": "PURPOSE", "@NlmCategory": "OBJECTIVE"}, {"#text": "An open AI model (ChatGPT) was used to answer 23 commonly asked questions from patients on shoulder stabilization surgery. These answers were evaluated for medical accuracy, quality, and readability using The JAMA Benchmark criteria, DISCERN score, Flesch-Kincaid Reading Ease Score (FRES) & Grade Level (FKGL).", "@Label": "METHODS", "@NlmCategory": "METHODS"}, {"#text": "The JAMA Benchmark criteria score was 0, which is the lowest score, indicating no reliable resources cited. The DISCERN score was 60, which is considered a good score. The areas that open AI model did not achieve full marks were also related to the lack of available source material used to compile the answers, and finally some shortcomings with information not fully supported by the literature. The FRES was 26.2, and the FKGL was considered to be that of a college graduate.", "@Label": "RESULTS", "@NlmCategory": "RESULTS"}, {"#text": "There was generally high quality in the answers given on questions relating to shoulder stabilization surgery, but there was a high reading level required to comprehend the information presented. However, it is unclear where the answers came from with no source material cited. It is important to note that the ChatGPT software repeatedly references the need to discuss these questions with an orthopaedic surgeon and the importance of shared discussion making, as well as compliance with surgeon treatment recommendations.", "@Label": "CONCLUSION", "@NlmCategory": "CONCLUSIONS"}, {"#text": "As shoulder instability is an injury that predominantly affects younger individuals who may use the Internet for information, this study shows what information patients may be getting online.", "@Label": "CLINICAL RELEVANCE", "@NlmCategory": "CONCLUSIONS"}], "CopyrightInformation": "Copyright \u00a9 2023 Arthroscopy Association of North America. Published by Elsevier Inc. All rights reserved."}, "ArticleDate": {"@DateType": "Electronic", "Day": "09", "Month": "08", "Year": "2023"}, "ArticleTitle": "Evaluation High-Quality of Information from ChatGPT (Artificial Intelligence-Large Language Model) Artificial Intelligence on Shoulder Stabilization Surgery.", "AuthorList": {"@CompleteYN": "Y", "Author": [{"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A.. Electronic address: eoghan.hurley@duke.edu."}, "ForeName": "Eoghan T", "Initials": "ET", "LastName": "Hurley"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Bryan S", "Initials": "BS", "LastName": "Crook"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Samuel G", "Initials": "SG", "LastName": "Lorentz"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Richard M", "Initials": "RM", "LastName": "Danilkowicz"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Brian C", "Initials": "BC", "LastName": "Lau"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Dean C", "Initials": "DC", "LastName": "Taylor"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Jonathan F", "Initials": "JF", "LastName": "Dickens"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Oke", "Initials": "O", "LastName": "Anakwenze"}, {"@ValidYN": "Y", "AffiliationInfo": {"Affiliation": "Duke University, Durham, North Carolina, U.S.A."}, "ForeName": "Christopher S", "Initials": "CS", "LastName": "Klifto"}]}, "ELocationID": [{"#text": "S0749-8063(23)00642-4", "@EIdType": "pii", "@ValidYN": "Y"}, {"#text": "10.1016/j.arthro.2023.07.048", "@EIdType": "doi", "@ValidYN": "Y"}], "Journal": {"ISOAbbreviation": "Arthroscopy", "ISSN": {"#text": "1526-3231", "@IssnType": "Electronic"}, "JournalIssue": {"@CitedMedium": "Internet", "PubDate": {"Day": "09", "Month": "Aug", "Year": "2023"}}, "Title": "Arthroscopy : the journal of arthroscopic & related surgery : official publication of the Arthroscopy Association of North America and the International Arthroscopy Association"}, "Language": "eng", "PublicationTypeList": {"PublicationType": {"#text": "Journal Article", "@UI": "D016428"}}}, "CitationSubset": "IM", "DateRevised": {"Day": "23", "Month": "08", "Year": "2023"}, "MedlineJournalInfo": {"Country": "United States", "ISSNLinking": "0749-8063", "MedlineTA": "Arthroscopy", "NlmUniqueID": "8506498"}, "PMID": {"#text": "37567487", "@Version": "1"}}, "PubmedData": {"ArticleIdList": {"ArticleId": [{"#text": "37567487", "@IdType": "pubmed"}, {"#text": "10.1016/j.arthro.2023.07.048", "@IdType": "doi"}, {"#text": "S0749-8063(23)00642-4", "@IdType": "pii"}]}, "History": {"PubMedPubDate": [{"@PubStatus": "received", "Day": "29", "Month": "3", "Year": "2023"}, {"@PubStatus": "revised", "Day": "27", "Month": "6", "Year": "2023"}, {"@PubStatus": "accepted", "Day": "28", "Month": "7", "Year": "2023"}, {"@PubStatus": "pubmed", "Day": "12", "Hour": "10", "Minute": "42", "Month": "8", "Year": "2023"}, {"@PubStatus": "medline", "Day": "12", "Hour": "10", "Minute": "42", "Month": "8", "Year": "2023"}, {"@PubStatus": "entrez", "Day": "11", "Hour": "19", "Minute": "27", "Month": "8", "Year": "2023"}]}, "PublicationStatus": "aheadofprint"}}}}, "PMC": null, "PMID": "37567487", "Published": null, "QueryTranslation": "\"large language model\"[Title] AND \"Benchmark\"[Title/Abstract]", "ReferenceCrawlerDeep": 0, "References": null, "SourceBank": 1, "State": -1, "Title": null, "Topics": null}}}